# -*- coding: utf-8 -*-
"""Untitled8(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1piXfzy6vwkAMl6pS5WLrJybOGrm4u6Rp
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("sovitrath/diabetic-retinopathy-224x224-2019-data")

print("Path to dataset files:", path)

import os

def walk_through(dir_path):
  for dirpath, dirnames, filenames in os.walk(dir_path):
    print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.")

walk_through(path)

import os
import torch
import torchvision.transforms as transforms
from torchvision import datasets, models
from torch.utils.data import DataLoader, random_split, WeightedRandomSampler
import torch.nn as nn
import torch.optim as optim
from tqdm.auto import tqdm
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_auc_score
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

DATA_DIR = "/root/.cache/kagglehub/datasets/sovitrath/diabetic-retinopathy-224x224-2019-data/versions/4/colored_images"
full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform)

# Calculate class weights for handling imbalance
class_counts = [0] * len(full_dataset.classes)
for _, index in full_dataset.samples:
    class_counts[index] += 1

total_samples = sum(class_counts)
class_weights = [total_samples / (len(class_counts) * count) for count in class_counts]
sample_weights = [class_weights[label] for _, label in full_dataset.samples]

device = "cuda" if torch.cuda.is_available() else "cpu"

device

sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(sample_weights),
    replacement=True
)

class_weights_tensor = torch.FloatTensor(class_weights).to(device)

train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
generator = torch.Generator().manual_seed(42)
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)

val_dataset.dataset.transform = val_transform

train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    sampler=sampler if train_dataset.dataset is full_dataset else None,  # Check for object identity
    num_workers=4,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=32,
    shuffle=False,
    num_workers=4,
    pin_memory=True
)

import torchvision

weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
model = torchvision.models.efficientnet_b0(weights=weights).to(device)

for i, (name, param) in enumerate(model.features.named_parameters()):
    if int(name.split('.')[0]) < 3:  # Freeze only the first 3 blocks
        param.requires_grad = False

full_dataset.classes

torch.manual_seed(42)
torch.cuda.manual_seed(42)


output_shape = len(full_dataset.classes)
model.classifier = nn.Sequential(
    nn.Dropout(p=0.3, inplace=True),
    nn.Linear(in_features=1280, out_features=output_shape, bias=True)
)

model.to(device)

import torch.nn as nn
import torch.optim as optim

criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)


optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.1,
    patience=2,
    verbose=True
)

import torch

from tqdm.auto import tqdm
from typing import Dict, List, Tuple

def train_step(model: torch.nn.Module,
               dataloader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               device: torch.device) -> Tuple[float, float]:



    model.train()


    train_loss, train_acc = 0, 0

    for batch, (X, y) in enumerate(dataloader):

        X, y = X.to(device), y.to(device)

        y_pred = model(X)


        loss = loss_fn(y_pred, y)
        train_loss += loss.item()


        optimizer.zero_grad()


        loss.backward()


        optimizer.step()


        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)
        train_acc += (y_pred_class == y).sum().item()/len(y_pred)


    train_loss = train_loss / len(dataloader)
    train_acc = train_acc / len(dataloader)
    return train_loss, train_acc

def test_step(model: torch.nn.Module,
              dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              device: torch.device) -> Tuple[float, float]:


    model.eval()


    test_loss, test_acc = 0, 0


    with torch.inference_mode():

        for batch, (X, y) in enumerate(dataloader):

            X, y = X.to(device), y.to(device)



            test_pred_logits = model(X)


            loss = loss_fn(test_pred_logits, y)
            test_loss += loss.item()

            test_pred_labels = test_pred_logits.argmax(dim=1)
            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))


    test_loss = test_loss / len(dataloader)
    test_acc = test_acc / len(dataloader)
    return test_loss, test_acc

def compute_metrics(y_true, y_pred, y_prob, classes):
    """
    Compute precision, recall, F1-score, and confusion matrix
    """
    # Convert tensors to numpy arrays if needed
    if isinstance(y_true, torch.Tensor):
        y_true = y_true.cpu().numpy()
    if isinstance(y_pred, torch.Tensor):
        y_pred = y_pred.cpu().numpy()
    if isinstance(y_prob, torch.Tensor):
        y_prob = y_prob.cpu().numpy()

    # Calculate precision, recall, F1
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='weighted', zero_division=0
    )

    # Create confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Calculate ROC AUC for multi-class (one-vs-rest)
    try:
        auc = roc_auc_score(
            np.eye(len(classes))[y_true],
            y_prob,
            multi_class='ovr',
            average='weighted'
        )
    except:
        auc = 0  # Fall back if there are issues with the AUC calculation

    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm,
        'auc': auc
    }

def plot_metrics(history, save_dir='./plots'):
    """
    Plot training metrics
    """
    os.makedirs(save_dir, exist_ok=True)

    # Plot loss
    plt.figure(figsize=(10, 5))
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.savefig(f'{save_dir}/loss_plot.png')

    # Plot accuracy
    plt.figure(figsize=(10, 5))
    plt.plot(history['train_acc'], label='Train Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.savefig(f'{save_dir}/accuracy_plot.png')

    # Plot F1 score
    plt.figure(figsize=(10, 5))
    plt.plot(history['val_f1'], label='Validation F1')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.title('Validation F1 Score')
    plt.legend()
    plt.savefig(f'{save_dir}/f1_plot.png')

    # Plot latest confusion matrix
    if 'confusion_matrix' in history:
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            history['confusion_matrix'],
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=full_dataset.classes,
            yticklabels=full_dataset.classes
        )
        plt.xlabel('Predicted Labels')
        plt.ylabel('True Labels')
        plt.title('Confusion Matrix')
        plt.savefig(f'{save_dir}/confusion_matrix.png')

def train_and_validate(model,
                       train_loader,
                       val_loader,
                       criterion,
                       optimizer,
                       scheduler,
                       device,
                       num_epochs=50,
                       patience=5,
                       checkpoint_dir='./checkpoints'):
    """
    Train and validate the model with early stopping and checkpointing.
    Enhanced with additional metrics tracking.
    """
    # Create checkpoint directory if it doesn't exist
    os.makedirs(checkpoint_dir, exist_ok=True)

    # Training history tracking
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'val_precision': [],
        'val_recall': [],
        'val_f1': [],
        'val_auc': [],
        'learning_rates': []
    }

    # Early stopping variables
    best_val_f1 = 0
    epochs_no_improve = 0

    # Training loop
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss, train_acc = 0, 0
        all_train_preds, all_train_labels = [], []

        train_progress_bar = tqdm(train_loader,
                                  desc=f'Epoch {epoch+1}/{num_epochs}',
                                  unit='batch')

        for batch, (X, y) in enumerate(train_progress_bar):
            X, y = X.to(device), y.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(X)
            loss = criterion(outputs, y)

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            # Compute metrics
            train_loss += loss.item()
            train_pred = torch.argmax(torch.softmax(outputs, dim=1), dim=1)
            train_acc += (train_pred == y).float().mean().item()

            # Store predictions and labels for metrics calculation
            all_train_preds.extend(train_pred.cpu().numpy())
            all_train_labels.extend(y.cpu().numpy())

            # Update progress bar
            train_progress_bar.set_postfix({
                'Train Loss': loss.item(),
                'Train Acc': (train_pred == y).float().mean().item()
            })

        # Average epoch metrics
        train_loss /= len(train_loader)
        train_acc /= len(train_loader)

        # Validation phase
        model.eval()
        val_loss, val_acc = 0, 0
        all_val_preds, all_val_labels = [], []
        all_val_probs = []

        with torch.inference_mode():
            for X, y in val_loader:
                X, y = X.to(device), y.to(device)

                outputs = model(X)
                loss = criterion(outputs, y)

                val_loss += loss.item()
                probs = torch.softmax(outputs, dim=1)
                val_pred = torch.argmax(probs, dim=1)
                val_acc += (val_pred == y).float().mean().item()

                # Store predictions, probabilities and labels for metrics calculation
                all_val_preds.extend(val_pred.cpu().numpy())
                all_val_labels.extend(y.cpu().numpy())
                all_val_probs.extend(probs.cpu().numpy())

        # Average validation metrics
        val_loss /= len(val_loader)
        val_acc /= len(val_loader)

        # Calculate additional metrics
        val_metrics = compute_metrics(
            np.array(all_val_labels),
            np.array(all_val_preds),
            np.array(all_val_probs),
            full_dataset.classes
        )

        # Store history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_precision'].append(val_metrics['precision'])
        history['val_recall'].append(val_metrics['recall'])
        history['val_f1'].append(val_metrics['f1'])
        history['val_auc'].append(val_metrics['auc'])
        history['learning_rates'].append(optimizer.param_groups[0]['lr'])

        # Store latest confusion matrix
        history['confusion_matrix'] = val_metrics['confusion_matrix']

        # Print epoch summary
        print(f'\nEpoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')
        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')
        print(f'Val Precision: {val_metrics["precision"]:.4f}, Val Recall: {val_metrics["recall"]:.4f}')
        print(f'Val F1: {val_metrics["f1"]:.4f}, Val AUC: {val_metrics["auc"]:.4f}')
        print(f'Current LR: {optimizer.param_groups[0]["lr"]}')

        # Update learning rate scheduler
        scheduler.step(val_loss)

        # Early stopping and model checkpointing - now using F1 score
        if val_metrics['f1'] > best_val_f1:
            best_val_f1 = val_metrics['f1']
            epochs_no_improve = 0

            # Save best model
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_f1': best_val_f1,
                'val_metrics': val_metrics,
                'class_mapping': full_dataset.classes
            }, os.path.join(checkpoint_dir, 'best_model.pth'))
            print(f'Best model saved with F1: {best_val_f1:.4f}\n')
        else:
            epochs_no_improve += 1
            print(f'No improvement for {epochs_no_improve} epochs.\n')

        # Early stopping
        if epochs_no_improve >= patience:
            print(f'Early stopping triggered after {epoch+1} epochs')
            break

        # Plot metrics every 5 epochs
        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:
            plot_metrics(history)

    # Plot final metrics
    plot_metrics(history)

    # Generate and save a classification report
    print("\nClassification Report:")
    metrics_df = pd.DataFrame({
        'Epoch': list(range(1, len(history['val_acc']) + 1)),
        'Train Loss': history['train_loss'],
        'Val Loss': history['val_loss'],
        'Train Acc': history['train_acc'],
        'Val Acc': history['val_acc'],
        'Val Precision': history['val_precision'],
        'Val Recall': history['val_recall'],
        'Val F1': history['val_f1'],
        'Val AUC': history['val_auc'],
        'Learning Rate': history['learning_rates']
    })
    metrics_df.to_csv('./checkpoints/training_metrics.csv', index=False)
    print(metrics_df.tail())

    return history

class_counts = [0] * len(full_dataset.classes)
for _, index in full_dataset.samples:
    class_counts[index] += 1

train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
generator = torch.Generator().manual_seed(42)
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)

val_dataset.dataset.transform = val_transform

train_dataset.samples = [full_dataset.samples[i] for i in train_dataset.indices]
train_dataset.targets = [s[1] for s in train_dataset.samples]
val_dataset.samples = [full_dataset.samples[i] for i in val_dataset.indices]
val_dataset.targets = [s[1] for s in val_dataset.samples]

history = train_and_validate(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    device=device,
    num_epochs=15,  # Increased from 10
    patience=5      # Increased from 3
)

torch.save(model.state_dict(), 'model.pth')
torch.save(model, 'model_full.pth')

